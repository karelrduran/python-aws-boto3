{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d036bb24",
   "metadata": {},
   "source": [
    "## Use case 4 - From S3 To DynamoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3565098",
   "metadata": {},
   "source": [
    "In this case, we are going to retrieve movie data from a public dataset available at: https://www.kaggle.com/sankha1998/tmdb-top-10000-popular-movies-dataset\n",
    "\n",
    "The content of the dataset is similar to.:\n",
    "\n",
    "#### TMDb_updated.CSV\n",
    "\n",
    ">`\n",
    ",title,overview,original_language,vote_count,vote_average\n",
    "0,Ad Astra,\"The near future...\",en,2853,5.9\n",
    "1,Bloodshot,\"After he ...\",en,1349,7.2\n",
    "2,Bad Boys for Life,\"Marcus and Mike ...\",en,2530,7.1\n",
    "`\n",
    "\n",
    "Once TMDb_updated.csv is downloaded, we will load the information into S3 within the bucket. For this case, instead of loading all the data from the dataset into our NoSQL table, we will put in DynamoDB the title, the average rating, and the overview only if they have received at least 10,000 votes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3e9ec",
   "metadata": {},
   "source": [
    "#### S3Select\n",
    "To perform this query from Python and automate the ETL process, we will use S3Select to retrieve the subset of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a3fcd",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\" alert alert-block alert-info\">\n",
    "<h3>S3Select vs AWS Athena</h3>\n",
    "This type of processing is more convenient to perform using AWS Athena, which does allow joining different datasets. S3Select only allows working with a single table.\n",
    "</div>\n",
    "\n",
    "To do this, we will use S3Select to execute the query<br>\n",
    "> `SELECT s.title, s.overview, s.vote_count, s.vote_average \n",
    " FROM s3object s \n",
    " WHERE cast(s.vote_count as int)> 10000`<br>\n",
    "\n",
    "and store the result in a new CSV within the same bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "879f8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75bbd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The credentials are stored into environment variables:\n",
    "\"\"\"\n",
    "\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY')\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY')\n",
    "\n",
    "AWS_REGION = os.getenv('AWS_REGION')\n",
    "\n",
    "BUCKET_NAME = 'my-k-new-bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97e8c917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'SQC9V51JYMBYNZ8K',\n",
       "  'HostId': 'WtPzxCG/nnzJzUyMDySnXl/gYH9BDCo+btuJbNgcNi7wkkKbp2IPoqiz36p3nVn9xsjtcSQ9bHE=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'WtPzxCG/nnzJzUyMDySnXl/gYH9BDCo+btuJbNgcNi7wkkKbp2IPoqiz36p3nVn9xsjtcSQ9bHE=',\n",
       "   'x-amz-request-id': 'SQC9V51JYMBYNZ8K',\n",
       "   'date': 'Fri, 21 Apr 2023 15:08:26 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"0e46f2eef741ecda8647ca3b748b1432\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"0e46f2eef741ecda8647ca3b748b1432\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3client = boto3.client(\n",
    "    's3', \n",
    "    region_name= AWS_REGION, \n",
    "    aws_access_key_id= AWS_ACCESS_KEY, \n",
    "    aws_secret_access_key= AWS_SECRET_KEY\n",
    ")\n",
    "\n",
    "# 1.- We perform the query using S3Select\n",
    "resp = s3client.select_object_content(\n",
    "    Bucket= BUCKET_NAME,\n",
    "    Key= 'TMDb_updated.csv',\n",
    "    ExpressionType='SQL',\n",
    "    Expression=\"SELECT s.title, s.overview, s.vote_count, s.vote_average \\\n",
    "                FROM s3object s \\\n",
    "                WHERE cast(s.vote_count as int)> 10000\",\n",
    "    InputSerialization={'CSV': {\"FileHeaderInfo\": \"USE\",\n",
    "                                'AllowQuotedRecordDelimiter': True},\n",
    "                        'CompressionType': 'NONE'},\n",
    "    OutputSerialization={'CSV': {}},\n",
    ")\n",
    "\n",
    "\n",
    "# 2.- We join the data we receive in streaming\n",
    "registros = [\"title,overview,vote_count,vote_average\\n\"]\n",
    "for evento in resp['Payload']:\n",
    "    if 'Records' in evento:\n",
    "        registros.append(evento['Records']['Payload'].decode())\n",
    "\n",
    "# 3.- We generate the content as a String\n",
    "file_str = ''.join(registros)\n",
    "\n",
    "# 4.- We create a new object in S3\n",
    "s3client.put_object(Body=file_str, Bucket= BUCKET_NAME,\n",
    "              Key= 'TMDb_filtered.CSV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15441487",
   "metadata": {},
   "source": [
    "### From S3 to DynamoDB\n",
    "Once the file is created in S3, we will load the data into DynamoDB. Since the dataset did not contain the movie release date, we will assign the year 2022 to all movies in our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7112dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# 1.- We read the file from S3 and put it into a DataFrame\n",
    "\n",
    "response = s3client.get_object(Bucket= BUCKET_NAME, Key= 'TMDb_filtered.CSV')\n",
    "movies_df = pd.read_csv(response['Body'], delimiter = ',')\n",
    "\n",
    "# 2.- We connect to DynamoDB\n",
    "dynamodb = boto3.resource(\n",
    "    'dynamodb', \n",
    "    region_name= AWS_REGION, \n",
    "    aws_access_key_id= AWS_ACCESS_KEY, \n",
    "    aws_secret_access_key= AWS_SECRET_KEY\n",
    ")\n",
    "table = dynamodb.Table('FilmsData')\n",
    "\n",
    "# 3.- We insert it into DynamoDB using a batch\n",
    "with table.batch_writer() as batch:\n",
    "    for index, fila in movies_df.iterrows():\n",
    "        Item = {\n",
    "            'year': 2022,\n",
    "            'title': str(fila['title']),\n",
    "            'info': {\n",
    "                'plot' : fila['overview'],\n",
    "                'rating' : Decimal(fila['vote_average']).quantize(Decimal('1.00'))\n",
    "            }\n",
    "        }\n",
    "        batch.put_item(Item=Item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da3ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
